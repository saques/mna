\documentclass[12pt, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\geometry{a4paper,margin=0.8in}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}
\usepackage{chngcntr}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\cfoot{Página \thepage}
\counterwithin*{equation}{subsection}

\begin{document}
	\title{Reconocimiento facial con PCA y KPCA \\ 
		   \large{\textsc{Métodos Numéricos Avanzados}} \\
		   \normalsize{\textsc{Instituto Tecnológico de Buenos Aires}}}
	\author{
		\textsc{Balaguer}, Pedro \\
		\texttt{55795}
		\and
		\textsc{Benítez}, Julián \\
		\texttt{56283}
		\and
		\textsc{Garrigó}, Mariano \\
		\texttt{54393}
		\and
		\textsc{Perazzo}, Matías \\
		\texttt{55024}
		\and
		\textsc{Saqués}, M. Alejo \\
		\texttt{56047} 
	}
	\date{}
	\maketitle
	
	\begin{abstract}
		Se han empleado técnicas matriciales, tales como \textit{Principal Component Analysis} y su variante, \textit{Kernel Principal component Analysis} para la clasificación de caras en sus respectivas clases, teniendo como referencia \textit{sets} de entrenamiento.  
		
		Asimismo, se han implementado algoritmos para el hallado de los autovectores y autovalores de una matriz, y se han explorado numerosos caminos para la descomposición QR, como así también para otras transformaciones que sirven de soporte para la implementación de un algoritmo QR mejorado.   
		
		Tras minuciosos análisis comparativos entre las dos metodologías implementadas, se pudo concluir que \textbf{KPCA} exhibe un comportamiento superior frente a \textbf{PCA}, dado que la proporción de aciertos del primer método demostró ser consistentemente superior a la del último.
		
		Por último, al realizar pruebas con \textit{kernels} polinomiales de diferentes grados, no ha podido afirmarse que a grados superiores la precisión de la clasificación sea, de mismo modo, superior.  
	\end{abstract}
	
	\paragraph{Palabras clave:} Descomposición QR, Transformación de Householder, Matriz de Hessemberg, Algoritmo QR, Corrimiento de Wilkinson, Matriz de Covarianza, \textit{Principal Component Analysis}, \textit{Kernel}, \textit{Support Vector Machines}.
	
	\section{Metodologías para reconocimiento de caras}
	
	\paragraph{} A continuación, se exhibirán las metodologías utilizadas para \textit{software} de reconocimiento de caras, utilizando técnicas requeridas por la Cátedra. 	
	
	\subsection{PCA}
	
	\paragraph{} Para la implementación de reconocimiento de caras utilizando \textbf{PCA}, el equipo se ha basado en la técnica de Saha y Bhattacharjee $($2013$)$ \cite{PCA}.
	
	\paragraph{} Sea $N$ la cantidad de individuos presentes en la base de datos, y sea $M$ la cantidad de imágenes de cada uno de ellos que se usarán a los efectos de \textit{entrenar} el \textit{software} de reconocimiento. Cada imagen tiene una dimensión $X\times Y$. Luego, se debe construir una matriz $T \in \mathbb{N}^{MN\times XY}$ que contenga todas las muestras a analizar. Luego, a la matriz $T$ se le resta fila por fila la \textit{cara promedio} $M$, es decir, la media de las filas de $T$. Esta matriz $C = T - M$ puede contener valores negativos, por lo que $C \in \mathbb{Z}^{MN\times XY}$.
	
	\paragraph{} El siguiente paso en el método de \textbf{PCA} es el hallado de los autovectores de la matriz de covarianza $S = C^{T}C$. Dichos autovectores son también referidos en la bibliografía por el nombre de \textit{autocaras o eigenfaces}. Es preciso observar que el producto $C^{T}C \in \mathbb{Z}^{XY\times XY}$, por lo cual, dado que $X\times Y$ es el número de píxeles de las imágenes, es de esperar que la matriz de covarianza $S$ sea de una dimensión excesivamente grande. Por ende, el objetivo es calcular las \textit{eigenfaces} de una forma computacionalmente más económica.
	
	\paragraph{} Nótese que la dimensión de la matriz $\widetilde{S} = CC^{T}$  $(\widetilde{S} \in \mathbb{Z}^{MN \times MN})$ está intrínsecamente asociada al tamaño de la muestra, ya sea del número de individuos y/o del de caras por individuo. Estos valores serán, en la mayoría de los casos, de una magnitud muy inferior a la cantidad de píxeles de las imágenes y, además, las posibilidades de manipularlos son mucho más amplias. A modo de ejemplo, las imágenes de la base de datos de prueba sugerida por la Cátedra \cite{dbf} tienen una dimensión de $92 \times 112$, por lo que la matriz de covarianza $S \in \mathbb{Z}^{10304\times 10304}$. Por el contrario, $\widetilde{S} \in \mathbb{Z}^{400 \times 400}$, en el caso de tomar todas las fotos disponibles de cada individuo.
	
	\paragraph{} Se puede mostrar que hay un procedimiento para calcular las \textit{eigenfaces} de $S$ a partir de los autovectores de $\widetilde{S}$:
	
	\begin{align}
		Sv {i} = C^{T}Cv_{i} &= \lambda_{i}v_{i}
	\end{align}
	
	\paragraph{} Como se ha visto, $S$ es de una dimensión potencialmente muy grande, por ende, se toman los autovectores de la matriz $\widetilde{S}$:
	
	\begin{align}
		\widetilde{S}u_{i} = CC^{T}u_{i} = \lambda_{i}u_{i}
	\end{align}
	
	\paragraph{} Multiplicando por izquierda a $(2)$ por $C^{T}$ se tiene que:
	
	\begin{align}
		C^{T}CC^{T}u_{i} = \lambda_{i}C^{T}u_{i} 
	\end{align}
	
	\paragraph{} Luego, se puede renombrar:
	
	\begin{align}
		C^{T}u_{i} = v_{i}
	\end{align}
	
	\paragraph{} Por lo que si $u_{i}$ es un autovector de $\widetilde{S}$, empleando $(4)$ se puede obtener el i-ésimo autovector de $S$, es decir, de la matriz de covarianza. 
	
	\paragraph{} Así pues, en este punto se tiene una colección de vectores $V = \left[v_{1}\quad\dots\quad v_{MN}\right]$. El interés es ahora proyectar las imágenes de la base de datos sobre estas \textit{eigenfaces}, para formar, por cada k-ésima imagen, un vector $\Omega_{k} = \left[\omega_{1}\quad\dots\quad\omega_{MN}\right]$, donde los $\omega_{i}$ representan el peso de cada \textit{eigenface} con respecto a dicha imagen:
	
	\begin{align}
		\omega_{i} = v_{i}^{T}P_{k}^{T}
	\end{align}
	
	\paragraph{} Siendo $P_{k}$ la k-ésima fila de $C$. Luego, una opción es promediar los $M$ $\Omega_{k_{s}}$ de cada individuo de la base de datos, así logrando un valor medio $\Omega_{M_{p}}$ por cada ejemplar $1 \le p \le N$, obteniendo un vector que defina la \textit{clase} de un individuo en particular. Otra posibilidad es no realizar ningún promedio alguno, entregando a la función clasificadora tantos vectores $\Omega$ como cantidad de imágenes por individuo haya. En la sección de resultados se analizarán las dos posibilidades en detalle.  
	
	\paragraph{} Luego, para una imagen de entrada $Z$ a clasificar $($dada en forma de columna$)$, se debe computar el peso de las \textit{eigenfaces} con respecto a la misma como se describe en $(5)$, teniendo la precaución de \textit{centralizar} la entrada, es decir, hacer la resta $\widetilde{Z} = Z - M^{T}$. Luego, reemplazando $P_{k}$ por $\widetilde{Z}$ en dicha ecuación, se calcula el vector $\Omega_{Z}$.
	
	\paragraph{} Finalmente, se toma la distancia euclídea $\epsilon_{p} = \lVert \Omega_{Z} - \Omega_{M_{p}} \rVert$ $-$ considerando el caso en el que los vectores $\Omega$ se promedien $-$ para cada $1 \le p \le N$, y se dice que el $p$ que corresponda al $\epsilon_{p}$ mínimo determina la clase a la que pertenece la imagen de entrada. 
	
	\paragraph{} Existe otra técnica para realizar el último cálculo, por medio del uso de \textit{Support Vector Machines}. En la implementación realizada, se ha utilizado la librería \verb|Sklearn| para dicho propósito. Su implementación de SVM espera, para el \textit{set} de entrenamiento, un arreglo de vectores que hagan las veces de los \textit{individuos}, y un arreglo de enteros que especifique a que clase pertenece cada uno de ellos. En este caso, el primer arreglo contendrá los $\Omega_{M_{p}}$. La entrada del método \verb|predict| será el $\Omega_{Z}$ a clasificar. 
	
	\subsection{Kernels y KPCA}
	
	\subsubsection{Funciones \textit{Kernel}}
	
	\paragraph{} Los métodos de \textit{kernel} son métodos de reconocimiento de patrones que permiten trabajar en altas dimensiones de forma implícita, es decir sin nunca computar coordenadas en dicho espacio. Utilizar estos métodos es computacionalmente mucho más eficiente a la hora de trabajar en dimensiones altas, y permite obtener datos analizables de forma lineal utilizando los denominados \textit{hiperplanos}. Este tipo de enfoque es llamado \textit{kernel trick} o truco de \textit{kernel} \cite{kern1}. 
	
	\paragraph{} Las funciones de \textit{kernel} son definidas de la siguiente manera:
	
	\begin{align}
		k : X \times X \to \mathbb{R}
	\end{align}
	
	\paragraph{} Para facilitar las operaciones, se puede notar:
	
	\begin{align}
		\phi : \mathbb{R}^{d} \to \mathbb{R}^{k} , k > d
	\end{align}
	
	\paragraph{} Siendo $\phi$ la función que \textit{mapea} los vectores $x$ a una dimensión superior utilizando combinaciones no lineales de las componentes de $x$. A modo de ejemplo, supóngase a $x \in \mathbb{R}^{d}$:
	
	\begin{align*}
		x = \left[x_{1}\quad x_{2}\quad\dots\quad x_{d}\right]^{T} \\ 
		\phi(x) = \left[x_{1}\quad x_{2}\quad x_{1}x_{2}\quad x_{1}^{2}\quad x_{1}x_{2}^{3} \quad \dots\right]^{T}
	\end{align*}
	
	\paragraph{} Luego, llámese \textit{función kernel} al producto interno de estas proyecciones de $x$:
	
	\begin{align}
		\kappa(x_{i}, x_{j}) = \phi(x_{i})\phi(x_{j})^{T}
	\end{align}
	
	\paragraph{} Existen numerosas funciones de \textit{kernel} conocidas. Algunos ejemplos son \cite{kernex}:
	
	\begin{itemize}
		\item \textit{Kernel} polinómico: \\ $\kappa(x,x')=(1+xx')^{p}$
		\item \textit{Kernel} gaussiano: \\ $\kappa(x,x')=\exp^{-V\lVert x-x'\rVert^{2}}$
		\item \textit{Kernel} de tangente hiperbólica: \\ $\kappa(x,x')=\tanh(xx'+\delta)$
	\end{itemize}
	
	\paragraph{} Cabe aclarar que dichas funciones de \textit{kernel} son un listado muy reducido de las existentes. Hallar la función que más se adecue al problema a resolver puede ser una tarea compleja.
	
	\subsubsection{\textit{Kernel} PCA}
	
	\paragraph{} Asumiendo que los datos son difícilmente linealizables, \textbf{KPCA} ofrece una gran ventaja frente a \textbf{PCA} ya que, como se explicó anteriormente, permitirá linealizar la información a través de una transformación a una dimensión superior. Para comenzar, una función de \textit{kernel} debe ser aplicada sobre los datos.
	
	\paragraph{} Considérese la siguiente expresión $\kappa(x_{i},x_{j})=\phi(x_{i})\phi(x_{j})^{T} = K_{ij}$.
	
	\paragraph{} Como no se tiene garantizado que los datos en el \textit{kernel} estén centrados, se recurrirá a centrarlos según la siguiente ecuación:
	
	\begin{align}
		\Phi_{i} := \Phi_{i} - \frac{1}{N}\sum_{k}\Phi_{k}
	\end{align}
	
	\paragraph{} Luego, la matriz de \textit{kernel} centrada resulta:
	
	\begin{align}
		K_{C} = K - 1_{\frac{1}{N}}\mathbf{K} - \mathbf{K}1_{\frac{1}{N}}  + 1_{\frac{1}{N}}\mathbf{K}1_{\frac{1}{N}}
	\end{align}
	
	\paragraph{} Donde $1_{\frac{1}{N}} \in \mathbb{R}^{N \times N}$ una matriz con todos valores $\frac{1}{N}$ $(\mathbf{K} \in \mathbb{R}^{N \times N})$.
	
	\paragraph{} Luego, de manera idéntica a \textbf{PCA}, se buscan hallar los autovalores y autovectores de una matriz, en este caso, de $K_{C}$. Esto puede considerarse equivalente a aplicar la técnica de \textbf{PCA} en la dimensión superior $\mathbb{R}^{K}$.
	
	\paragraph{} El ordenamiento de los autovalores y sus respectivos autovectores en orden decreciente permite seleccionar las componentes más significativas del análisis, para luego proyectar las imágenes sobre los mismos y generar las \textit{eigenfaces}. Un similar procedimiento de reducción puede realizarse en \textbf{PCA}.
	
	
	\section{Algoritmos de soporte}
	
	\paragraph{} A los efectos de implementar la metodología expuesta, es preciso contar con un método para el cálculo de los autovalores y autovectores de una matriz. A continuación, se expondrán los detalles de dicho algoritmo, el \textbf{algoritmo QR}, como así también los detalles de otras implementaciones necesarias para que éste funcione.
	
	\subsection{Algoritmo QR}
	
	\paragraph{} Sea $A \in \mathbb{R}^{N \times N}$ la matriz cuyos autovalores quieren determinarse. Sea $A_{k}=:Q_{k}R_{k}$ la descomposición QR de la matriz $A_{k}$, donde $Q_{k}$ es ortogonal y $R_{k}$ es triangular superior. Entonces el algoritmo QR se define de la siguiente forma:
	
	\begin{align}
		A_{0} &= A \\
		A_{k+1} &= R_{k}Q_{k}
	\end{align}
	
	\paragraph{} Nótese que, como los $Q_{k}$ son ortogonales, se cumple la siguiente igualdad:
	
	\begin{align*}
		A_{k+1} &= R_{k}Q_{k}  \\ &= Q^{-1}_{k}Q_{k}R_{k}Q_{k} \\ &= Q^{-1}_{k}A_{k}Q_{k}
	\end{align*}
	
	\paragraph{} Por ende, los $A_{k}$ son similares entre sí $\forall k\ge0$.
	
	\paragraph{} Siguiendo este procedimiento, para $k\to\inf$, la matriz $A_{k}$ converge a una forma triangular, lo que implica que sus autovalores se encuentran sobre la diagonal.
	
	\paragraph{} Un aspecto interesante de dicha convergencia es que sucede de forma \textbf{ordenada}, i.e. primero converge el valor $A_{N,N}$, luego $A_{N-1,N-1}$ y así sucesivamente. Por ende, la primer optimización que puede realizarse es la siguiente: 
	
	\paragraph{} Sea $i$ la i-ésima iteración del algoritmo. Luego, si $|A_{i+1_{j,j}} -A_{i_{j,j}}| \le \epsilon$ con $\epsilon \to 0$, entonces puede estimarse que el autovalor $A_{j,j}$ ha convergido. Luego, la fila y la columna j-ésimas pueden descartarse en las sucesivas iteraciones, para proceder el algoritmo con una matriz en $\mathbb{R}^{j-1 \times j-1}$. De esta forma, la complejidad computacional del algoritmo se ve reducida.
	
	\paragraph{} Otras posibles optimizaciones del algoritmo versan sobre el aceleramiento de su convergencia. Por ejemplo, es posible llevar la matriz $A_{0}$ a la forma de Hessemberg $H = Q^{-1}A_{0}Q$, siendo $H$ una matriz con $0_{s}$ por debajo de la subdiagonal. Considerando que mediante el algoritmo QR se converge a una matriz triangular, establecer $A_{0} = H$ acelera la convergencia de los autovalores.
	
	\paragraph{} Por último, a los efectos de acelerar la convergencia, se pueden introducir corrimientos o \textit{shifts} $\kappa_{k}$ en cada iteración, de la siguiente forma:
	
	\begin{align}
		A_{k} - \kappa_{k}I &=: Q_{k}R_{k} \\
		A_{k+1} &= R_{k}Q_{k} + \kappa_{k}I 
	\end{align}
	
	\paragraph{} Introduciendo los corrimientos, se preserva la similaridad entre todos los $A_{k}$. Por un lado, tenemos que:
	
	\begin{align}
		Q_{k}^{-1}(A_{k} - \kappa_{k}I)Q_{k} &= Q_{k}^{-1}Q_{k}R_{k}Q_{k}  \\ &= R_{k}Q_{k}
	\end{align}
	
	\paragraph{} Luego, reemplazando $R_{k}Q_{k}$ en $(7)$:
	
	\begin{align}
		A_{k+1} &= Q_{k}^{-1}(A_{k} - \kappa_{k}I)Q_{k} + \kappa_{k}I \\ &= Q_{k}^{-1}A_{k}Q_{k} - \kappa_{k}Q_{k}^{-1}Q_{k} + \kappa_{k}I  \\ &= Q_{k}^{-1}A_{k}Q_{k}  
	\end{align}
	
	\paragraph{} Por lo que $A_{k+1} \sim A_{k}$. Idealmente, en cada paso $\kappa_{k} = \lambda_{i}$, siendo $\lambda_{i}$ el autovalor al que converge el valor inferior derecho de la diagonal. Como precisamente el problema que se intenta resolver es obtener los autovalores de una matriz, se deberán utilizar heurísticas para aproximar dichos valores en cada iteración. Una heurística posible es la \textit{trivial}: tomar exactamente el extremo inferior derecho de la diagonal como $\kappa_{k}$. Existen otras heurísticas, tales como los corrimientos de Wilkinson, que buscan aproximar uno de los autovalores de la matriz de $2\times2$ de la esquina inferior derecha.
	
	\paragraph{} Para evaluar el comportamiento del algoritmo frente a diferentes tipos de corrimiento, se han tomado mediciones del tiempo de ejecución del algoritmo QR utilizando diferentes heurísticas para los \textit{shifts}. Para cada una de las dimensiones del cuadro siguiente, se han generado matrices aleatorias cuyos autovalores $-$ por medio de una transformación de similaridad $-$ sean exactamente los de $ \Lambda = \left[1\quad\dots\quad dim\right]$. En \ref{shcmp} se exhiben los resultados obtenidos.
	\begin{table}[h]
		\centering
		\label{shcmp}
		\begin{tabular}{@{}llll@{}}
			\toprule
			Dim. & Sin \textit{shifts} & Wilkinson & \textit{Trivial}  \\ \midrule
			10   & 0.022000           & 0.013000  & 0.007000 \\
			20   & 0.093000           & 1.154000  & 0.027000 \\
			30   & 0.212000           & 0.370000  & 0.071000 \\
			40   & 0.466000           & 0.262000  & 0.153000 \\
			50   & 0.859000           & 0.879000  & 0.260000 \\
			60   & 1.495000           & 0.569000  & 0.385000 \\
			70   & 2.061000           & 0.844000  & 0.627000 \\
			80   & 3.141000           & 4.478000  & 0.904000 \\
			90   & 4.212000           & 2.257000  & 1.543000 \\ \bottomrule
		\end{tabular}
		\caption{Comparación en tiempo de ejecución entre diferentes tipos de \textit{shifts}.}
	\end{table}
	
	\paragraph{} Contrariamente a lo que pudo haberse esperado, los corrimientos de Wilkinson implementados no solo demostraron no ser universalmente superiores a los corrimientos \textit{triviales}, sino que en algunos casos tuvieron una \textit{performance} incluso inferior al del algoritmo sin ningún tipo de corrimiento. Uno de los posibles motivos de este resultado podría estar relacionado a que el cómputo de los corrimientos de Wilkinson conlleva una serie de operaciones de punto flotante, tales como raíces cuadradas y divisiones. Por el contrario, los corrimientos \textit{triviales} son de cálculo inmediato.
	
	\paragraph{} Considerando los resultados expuestos, se optó finalmente por utilizar los \textit{shifts} \textit{triviales}.
	
	\subsection{Reflectores de Householder}
	
	\paragraph{} Tal como se ha mencionado en el caso de la matriz de Hessemberg, uno de los intereses que se tiene a menudo es la introducción de $0_{s}$ por debajo de la subdiagonal. Unas de las herramientas utilizadas para dicha caso, como así también para la descomposición QR de una matriz, son los reflectores de Householder.
	
	\paragraph{} Considérese la siguiente matriz simétrica $P = I - \mu vv^{T}$. El objetivo es encontrar un $\mu$ tal que $P$ sea, además, ortogonal. Para ello, debe cumplirse la igualdad $P^{T}P = I$ \cite{l9}.
	
	
	\begin{align}
		P^{T}P = I &= (I - \mu vv^{T})^{T}(I - \mu vv^{T}) \\
		 &= I - 2\mu vv^{T} + \mu^{2}vv^{T}vv^{T} \\
		 &= I - 2\mu vv^{T} + \mu^{2}(v^{T}v)vv^{T} \\
		 &= I + \mu(\mu v^{T}v - 2)vv^{T} \\
		0 &= \mu v^{T}v - 2 \\
		\mu &= \frac{2}{v^{T}v}
	\end{align}
	
	\paragraph{} Luego, sin perder generalidad, puede fijarse $v^{T}v = \lVert v \rVert _{2}^{2} = 1$, quedando la forma tradicional del reflector de Householder:
	
	\begin{align}
		P = I - 2vv^{T}
	\end{align}
	\paragraph{}Con $\lVert v \rVert _{2}^{2} = 1$.
	
	\subsection{Descomposición QR}
	
	\paragraph{} Siendo $A =: A_{0}$ una matriz cualquiera de $\mathbb{R}^{N\times M}$, se busca una descomposición $A = QR$ tal que $Q$ sea ortogonal y $R$ triangular superior. Utilizando los reflectores de Householder, el interés es encontrar una matriz $H$ de la forma $H = I - 2vv^{T}$ tal que se cumpla para algún vector $x$:
	
	\begin{align}
		Hx = \alpha e_{1}
	\end{align}
	\paragraph{}Con $e1 = \left[1\quad0\quad\dots\quad0\right]^{T}$.
	
	\paragraph{} Aplicando esta limitación y la definición del reflector, se puede llegar a que:
	
	\begin{align}
		v &= x + \lVert x \rVert _{2}e_{1} \\
		c &= \frac{2}{\lVert v \rVert _{2}^{2}} \\
		H &= I - cvv^{T}
	\end{align}
	\paragraph{}Con $H$ ortogonal.
	
	\paragraph{} La idea, pues, es generar matrices de Householder $H_{i}$ tales que $H_{1}A_{0}$ retorne una matriz $A_{1}$ con $0_{s}$ por debajo de la diagonal en la primera columna, $H_{2}A_{1}$ además agregue $0_{s}$ debajo de la diagonal en la segunda columna, y así sucesivamente, hasta lograr un producto $H_{M-1}H_{M-2}\dots H_{1}A = R$, tal que $R$ sea triangular superior. Los $H_{i}$ son de la siguiente forma:
	
	\begin{align}
	H_{i} = \begin{bmatrix} 
	I_{i-1} & 0 	   \\
	0 	   & \widetilde{H}_{i}         
	\end{bmatrix}
	\end{align}
	
	
	\paragraph{} Con $1 \le i \le M-1$. La matriz $\widetilde{H}_{i}$ cumple la condición $\widetilde{H}_{i}A_{i-1_{i:,i:}} = \alpha e_{1}$, con indexación al estilo de \textsc{Matlab}. Recuérdese que $A_{0} = A$, la matriz de entrada del algoritmo.
	
	\paragraph{} Nótese que, como los $\widetilde{H}_{i}$ son ortogonales, por la construcción en $(5)$ los $H_{i}$ son ortogonales, luego el producto $H_{M-1}H_{M-2}\dots H_{1} = Q^{T}$ es ortogonal. Por ende, se ha conseguido una matriz $R$ triangular superior, y una $Q$ ortogonal, tal que $QR = A$, por lo que se ha logrado la descomposición requerida. 
	
	\paragraph{} Se ha evaluado otro método para realizar descomposiciones QR, el cual involucra implementar \textit{rotaciones de Givens} para cada celda por debajo de la diagonal principal. Este método exhibe una mayor complejidad algorítmica, ya que hay que iterar sobre los $O(N^{2})$ casilleros por debajo de la diagonal, e introducir un similar orden de rotaciones, efectuando un idéntico número de multiplicaciones matriciales, una operación de orden cúbico. Con los reflectores de Householder, se recorren a lo sumo las $M-1$ columnas de la matriz, introduciendo la cantidad deseada de $0_{s}$ por columna en cada iteración.
	
	\begin{table}[]
		\centering
		\label{qrcmp}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Dim. & T. Givens & T. Householder \\ \midrule
			50        & 0.044000  & 0.004000       \\
			100       & 0.513000  & 0.014000       \\
			150       & 2.673000  & 0.046000       \\
			200       & 8.824000  & 0.148000       \\
			250       & 26.306000 & 0.259000       \\
			300       & 60.185000 & 0.485000       \\ \bottomrule
		\end{tabular}
		\caption{Comparación entre implementaciones de descomposición QR $($segundos$)$.}
	\end{table}
	
	\paragraph{} En \ref{qrcmp} se evidencia cómo incluso a dimensiones pequeñas de la matriz, la diferencia en tiempo de ejecución entre las implementaciones es sumamente apreciable.
	
	\subsection{Matriz de Hessemberg}
	
	\paragraph{} En la sección 1.1. se ha comentado acerca de los potenciales beneficios de transformar la matriz de entrada $A$ del algoritmo QR a una forma de Hessemberg superior, es decir con $0_{s}$ por debajo de la subdiagonal. A continuación, se abordarán los detalles de dicha transformación.
	
	\paragraph{} La reducción a una matriz de Hessemberg procede de manera similar a la descomposición QR por el método de Householder, con la diferencia que, en este caso, se busca generar una matriz $H \sim A$, donde $A \in \mathbb{R}^{N\times M}$ es la entrada del algoritmo. 
	
	\paragraph{} El objetivo es generar matrices de Householder $P_{i}$, con $ 1\le i \le M-2$, tal que $P_{1}A$ anule los elementos de la primera columna por debajo de la \textbf{subdiagonal}. Luego, como se busca generar una matriz similar a la entrada, se multiplica por derecha a este resultado por $P_{1}^{-1}$. Procediendo de esta forma hasta $i = M-2$ se obtiene $H = P_{M-2}\dots P_{1}AP_{1}^{-1}\dots P_{M-2}^{-1}$, con $H \sim A$. Un interesante ejemplo gráfico del procedimiento se puede observar en la referencia a continuación \cite{hess}. 
	

	
	\section{Resultados}
	
	\begin{table*}[t]
		\centering
		\begin{tabular}{@{}ll|ll|ll@{}}
			\multicolumn{2}{c|}{Parámetros} & \multicolumn{2}{c|}{DB. Cambridge}   & \multicolumn{2}{c}{DB. Propia}        \\
			N.Img.      & Img. P/Per.      & \multicolumn{1}{c|}{C/Prom.} & S/Prom. & \multicolumn{1}{c|}{C/Prom.} & S/Prom. \\ \midrule
			2           & 1                & .8                           & .8      & .8                           & .8      \\
			3           & 1                & 1                            & 1       & 1                            & 1       \\
			4           & 1                & .8                           & .8      & .6                           & .6      \\
			7           & 1                & 1                            & 1       & 1                            & 1       \\
			4           & 3                & .8                           & .8      & .4                           & .6      \\
			5           & 3                & .6                           & .6      & .6                           & .8      \\
			6           & 3                & .8                           & .8      & .8                           & .6      \\
			7           & 3                & .8                           & .8      & .6                           & .8      \\
			8           & 3                & 1                            & 1       & .2                           & .4      \\
			8           & 5                & 1                            & 1       & .6                           & .4      \\
			9           & 5                & .8                           & .8      & .6                           & .6      \\
			8           & 7                & .8                           & .8      & .4                           & .4      \\
			9           & 7                & .4                           & 1       & .4                           & .6     
		\end{tabular}
		\label{pca}
		\caption{\textbf{PCA:} Dos bases de datos diferentes}
	\end{table*}
	
	
	% Please add the following required packages to your document preamble:
	% \usepackage{booktabs}
	% \usepackage[normalem]{ulem}
	% \useunder{\uline}{\ul}{}
	\begin{table*}[]
		\centering
		\begin{tabular}{@{}ll|ll|ll@{}}
			\multicolumn{2}{c}{Parámetros} & \multicolumn{2}{c|}{Cambridge}                & \multicolumn{2}{c}{Propia}                    \\
			N.Img.      & Img. P/Per.      & \multicolumn{1}{l|}{Polin. O. 2} & Polin. O.4 & \multicolumn{1}{l|}{Polin. O. 2} & Polin. O. 4 \\ \midrule
			2           & 1                & .8                               & \textbf{1} & 1                                & 1           \\
			3           & 1                & 1                                & .8         & .8                               & \textbf{1}  \\
			4           & 1                & 1                                & 1          & .6                               & \textbf{.8} \\
			7           & 1                & 1                                & 1          & 1                                & 1           \\
			4           & 3                & 1                                & \textbf{1} & .6                               & .6          \\
			5           & 3                & .8                               & .8         & 1                                & 1           \\
			6           & 3                & 1                                & 1          & .8                               & .6    \\
			7           & 3                & 1                                & 1          & 1                                & 1           \\
			8           & 3                & .8                               & .8         & .8                               & .8          \\
			8           & 5                & 1                                & 1          & .8                               & .8          \\
			9           & 5                & 1                                & 1          & .8                               & .8          \\
			8           & 7                & .8                               & \textbf{1} & 1                                & 1           \\
			9           & 7                & 1                                & 1          & .8                               & .8         
		\end{tabular}
		\caption{\textbf{KPCA:} Comparación entre diferentes \textit{kernels} polinomiales}
		\label{kpca}
	\end{table*}
	
	\paragraph{} A continuación, se discutirán los resultados de la ejecución de ambas implementaciones, tomando como resultado de las mismas la proporción de aciertos por sobre el total de ejemplares. 
	
	\paragraph{} A la hora de realizar las pruebas, se ha tomado tanto una base de datos de construcción propia como así también la base de datos recomendada por la Cátedra \cite{dbf}. El objetivo de realizar esto es analizar el comportamiento de los algoritmos frente a bases de datos de diferente calidad. 
	
	\paragraph{} A modo de aclaración, en cada uno de los casos que se realizaron se ha buscado clasificar imágenes \textbf{no incluidas} en el \textit{set} de entrenamiento, a los efectos de evaluar las capacidades de los algoritmos de reconocer imágenes con rasgos distintos a los conocidos.
	
	\paragraph{Notas sobre el procedimiento:} Para cada una de las tablas, se están tomando $Img. P/Per$ imágenes de \textbf{5 individuos} diferentes. $N.Img.$ representa el número de imagen de cada individuo que se está tomando en cada ocasión. Luego, para cada base de datos analizada, y para cada configuración del par $(N. Img. - Img. P/Per.)$, se están clasificando 5 imágenes, usando un \textit{set} de entrenamiento de un tamaño de $5*Img. P/Per.$.
	
	
	\subsection{PCA}
	
	\paragraph{} Tal como se ha planteado en la descripción del método de reconocimiento de caras por \textit{Principal Component Analysis}, una de las opciones es \textbf{no} promediar las proyecciones de las imágenes del \textit{set} de entrenamiento con las \textit{eigenfaces}. Como se puede observar en \ref{pca}, y exceptuando contados casos, no se evidencian diferencias considerables entre el método que realiza los promedios y el que no. Existen algunos casos, tales como el $[N. Img = 9; Img. P/Per = 7]$, en el que, tomando ambas bases de datos, el desempeño del algoritmo que promedia los $\Omega$ es considerablemente inferior al del que no realiza dicho promedio. 
	
	\paragraph{} Sin embargo, el aspecto quizás más destacable que puede observarse en \ref{pca} es la diferencia de resultados entre la aplicación de los algoritmos sobre la base de datos sugerida por la Cátedra y la provista por el equipo. A pesar de que se han tomado medidas para minimizar los posibles errores por imágenes poco claras $($procurar un ambiente con abundante iluminación, centrar la captura y usar un fondo estandarizado$)$, se puede observar que en cada uno de los casos, los resultados con la primera base de datos son como mínimo igual de precisos que los de la segunda. Posibles soluciones a este hallazgo incluyen tanto ampliar la base de datos propia, como así también procurar un entorno profesional para la captura de imágenes.  
	
	\paragraph{} Otro aspecto que puede notarse en \ref{pca} es que, a medida que aumenta el tamaño del \textit{set} de entrenamiento, la precisión no necesariamente aumenta de igual forma. Esto se puede observar en el caso de la imagen $7$: tomando solo $1$ muestra por individuo para el entrenamiento, todas las imágenes son correctamente clasificadas. Sin embargo, tomando $3$ muestras por individuo, la precisión disminuye en todos los casos.
		
	\subsection{Kernel-PCA}
	
	\paragraph{} En \ref{kpca} puede evidenciarse como, efectivamente, la técnica del \textit{kernel} arroja resultados mejorados con respecto al \textbf{PCA} sin mejoras de tal tipo. 
	
	\paragraph{} La mejora de los resultados resulta clara en el caso de la base de datos sugerida por la Cátedra, donde prevalecen, salvo contadas excepciones, los aciertos absolutos. Dicha mejoría se evidencia también en la base de datos provista por el equipo: en casos donde la proporción de aciertos era igual o inferior, en \textbf{PCA} a $0.6$, en \textbf{KPCA} las mismas proporciones nunca son menores a $0.8$. Este último ejemplo podría estar indicando el accionar del \textit{kernel}: la mayor cantidad de datos del \textit{set} de entrenamiento pudo ser separada más correctamente en una dimensión elevada.
	
	\paragraph{} Como puede notarse, los cambios de precisión utilizando un polinomio de grado $4$ no son necesariamente positivos tomando como punto de comparación un \textbf{KPCA} que utiliza un \textit{kernel} polinomial de grado $2$. En ambas bases de datos pueden observarse tanto aumentos del número de clasificaciones correctas, como así también decrecimiento en las mismas. Como no se pudo corroborar un aumento generalizado de la precisión, podría inferirse que polinomios de grado superior \textit{no necesariamente} garantizan mejores resultados.
	
	\section{Conclusión}
	
	\paragraph{} Se pudo corroborar la mejora que supone el método de \textit{Kernel Principal Component Analysis} por sobre su versión sin la utilización de funciones de \textit{kernel} para elevar la dimensión de la muestra. Sin embargo, no se ha podido concluir que un \textit{kernel} polinomial de grado mayor a $2$ exhiba mejores resultados que uno de dicho grado a la hora de aplicar \textbf{PCA}.
	
	\paragraph{} Por otro lado, en lo que respecta al algoritmo QR, se han analizado diversos \textit{shifts}, determinándose que los corrimientos de Wilkinson no necesariamente generan una convergencia más rápida frente a un corrimiento trivial. 
	
	
	\newpage
	\begin{thebibliography}{9}
	
		\bibitem{PCA}
		\textsc{International Journal of Emerging Technology and Advanced Engineering}, \textbf{Face 
		Recognition Using Eigenfaces}, Rajib Saha, Debotosh Bhattacharjee,
		\url{http://www.ijetae.com/files/Volume3Issue5/IJETAE_0513_14.pdf}
		
		\bibitem{dbf}
		\textsc{Cambridge University Computer Laboratory}, \textbf{The Database of Faces},
		\url{http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html}
	
		\bibitem{kern1}
		\textsc{Sebastian Raschka},
		\url{http://sebastianraschka.com/Articles/2014_kernel_pca.html#kernel-functions-and-the-kernel-trick}
		
		\bibitem{kernex}
		\textsc{University of Haifa}, \textbf{Kernel PCA}
		\url{http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf}
	
		\bibitem{l9}
		\textsc{University of Southern Mississippi}, \textbf{QR Factorization},
		\url{http://www.math.usm.edu/lambers/mat610/sum10/lecture9.pdf}
		
		\bibitem{hess}
		\textsc{Technische Universität Berlin}, \textbf{Hessemberg matrix visualization},
		\url{http://www3.math.tu-berlin.de/Vorlesungen/SS11/NumMath2/Materials/hessenberg_eng.pdf}
		
		\bibitem{qr}
		\textsc{ETH Zurich}, \textbf{QR Algoritm},
		\url{http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter4.pdf}
	
	\end{thebibliography}
	
\end{document}